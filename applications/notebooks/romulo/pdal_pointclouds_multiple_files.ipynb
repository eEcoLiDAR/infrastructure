{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PointClouds in Spark using PDAL\n",
    "We calculate the Normalized Height for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.pointcloud.spark.Extent3D\n",
    "import geotrellis.pointcloud.spark.io.hadoop.HadoopPointCloudRDD\n",
    "import geotrellis.pointcloud.spark.io.s3.S3PointCloudRDD\n",
    "import geotrellis.pointcloud.spark.tiling.CutPointCloud\n",
    "import geotrellis.raster.GridExtent\n",
    "import geotrellis.spark.SpatialKey\n",
    "import geotrellis.spark.io.hadoop.HadoopCollectionLayerReader\n",
    "import geotrellis.spark.tiling.LayoutDefinition\n",
    "import geotrellis.vector.Extent\n",
    "import io.pdal.Pipeline\n",
    "import io.pdal.pipeline.{HagFilter, EigenValuesFilter, LasRead, LasWrite, PythonFilter, Read}\n",
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import spire.syntax.cfor._\n",
    "import scalaz.stream.Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sparkContext = org.apache.spark.SparkContext@9eca6f5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://145.100.58.104.surf-hosted.nl:4040)\" target=\"new_tab\">Spark UI: app-20180521191653-0000</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark app-20180521191653-0000: Some(http://145.100.58.104.surf-hosted.nl:4040)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val sparkContext = sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tiles_path = /data/local/home/ecolidar/\n",
       "tile = C_25EZ2\n",
       "laz_filepath = /data/local/home/ecolidar/C_25EZ2.laz\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/data/local/home/ecolidar/C_25EZ2.laz"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tiles_path = \"/data/local/home/ecolidar/\"\n",
    "val tile = \"C_25EZ2\"\n",
    "val laz_filepath = tiles_path + tile + \".laz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read local and save the result back to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "las_pipelineExpr = List(LasRead(/data/local/home/ecolidar/C_25EZ2.laz,None,None,None,None,None,readers.las), HagFilter(filters.hag), LasWrite(/data/local/home/ecolidar/C_25EZ2_hag.laz,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,writers.las))\n",
       "las_pipeline = io.pdal.Pipeline@155c10af\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "io.pdal.Pipeline@155c10af"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val las_pipelineExpr = LasRead(laz_filepath) ~ HagFilter() ~ FerryFilter(dimensions = \"Z=HeightAboveGround\") ~ LasWrite(tiles_path + tile + \"_hag.laz\")\n",
    "val las_pipelineExpr = LasRead(laz_filepath) ~ HagFilter() ~ LasWrite(tiles_path + tile + \"_hag.laz\")\n",
    "val las_pipeline: Pipeline = las_pipelineExpr.toPipeline\n",
    "las_pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if the saved file has HeightAboveGround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HDFS and apply HagFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laz_path = /user/hadoop/ahn3/C_25EZ2.las\n",
       "pipelineExpr = List(Read(local,None,None,None), HagFilter(filters.hag))\n",
       "rdd_laz = NewHadoopRDD[56] at newAPIHadoopRDD at HadoopPointCloudRDD.scala:76\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NewHadoopRDD[56] at newAPIHadoopRDD at HadoopPointCloudRDD.scala:76"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val laz_path = new Path(\"/user/hadoop/ahn3/C_25EZ2.las\")\n",
    "val pipelineExpr = Read(\"local\") ~ HagFilter() //~ FerryFilter(dimensions = \"HeightAboveGround=Z\")\n",
    "val rdd_laz = HadoopPointCloudRDD(laz_path, options = HadoopPointCloudRDD.Options(pipeline = pipelineExpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if HeightAboveGround shows up in the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(\"{\n",
       "  \"schema\":\n",
       "  {\n",
       "    \"dimensions\":\n",
       "    [\n",
       "      {\n",
       "        \"name\": \"X\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Y\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Z\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Intensity\",\n",
       "        \"size\": 2,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"ReturnNumber\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"NumberOfReturns\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"ScanDirectionFlag\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"EdgeOfFlightLine\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigne...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "  \"schema\":\n",
       "  {\n",
       "    \"dimensions\":\n",
       "    [\n",
       "      {\n",
       "        \"name\": \"X\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Y\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Z\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Intensity\",\n",
       "        \"size\": 2,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"ReturnNumber\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"NumberOfReturns\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"ScanDirectionFlag\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"EdgeOfFlightLine\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"Classification\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"ScanAngleRank\",\n",
       "        \"size\": 4,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"UserData\",\n",
       "        \"size\": 1,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"PointSourceId\",\n",
       "        \"size\": 2,\n",
       "        \"type\": \"unsigned\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"GpsTime\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      },\n",
       "      {\n",
       "        \"name\": \"HeightAboveGround\",\n",
       "        \"size\": 8,\n",
       "        \"type\": \"floating\"\n",
       "      }\n",
       "    ]\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_laz.cache()\n",
    "rdd_laz.map{ case (h, i) => h.schema}.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_laz.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewHadoopRDD[0] at newAPIHadoopRDD at HadoopPointCloudRDD.scala:76"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_laz.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[18] at repartition at <console>:45"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_laz.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_laz.flatMap(_._2).getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>(995 + 5) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45213"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointCloudTiled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointCloudTiled.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the points into RDD and compare Z with NormalizedHeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "points = MapPartitionsRDD[82] at filter at <console>:95\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[82] at filter at <console>:95"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val points :RDD[(Double, Double, Double, Byte, Double)] = rdd_laz.flatMap(_._2).mapPartitions{ _.map { packedPoints =>\n",
    "    var res = new Array[(Double, Double, Double, Byte, Double)](packedPoints.length)\n",
    "    cfor(0)(_ < packedPoints.length, _ + 1) { i =>\n",
    "        res(i) = (packedPoints.getX(i), packedPoints.getY(i), packedPoints.getZ(i), packedPoints.getByte(i, dim = \"Classification\"), packedPoints.getDouble(i, dim = \"HeightAboveGround\"))\n",
    "    }\n",
    "    res\n",
    "}}.flatMap( m => m).filter(_._4 != 2)//.filter(_._3 > 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15785689"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(125000.0,487503.61100000003,10.738,6,7.497999999999999), (125000.0,487512.52400000003,10.867,6,7.644000000000001), (125000.0,487522.228,10.901,6,9.167), (125000.0,488181.435,-0.47500000000000003,9,-0.8190000000000001), (125000.0,488317.335,-0.466,9,-0.020000000000000018), (125000.0,488351.775,-0.47400000000000003,9,-0.028000000000000025), (125000.0,488396.354,-0.47900000000000004,9,-0.03300000000000003), (125000.0,488475.981,-0.481,9,-0.08999999999999997), (125000.0,488810.282,8.955,1,8.016), (125000.0,488861.316,4.473,1,3.635)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.sortBy{ case(x, y, z, c, zh) => (x,y)}.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile the pointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extent = Extent(0.0, 0.0, 10.0, 5.0)\n",
       "gridExtent = GridExtent(Extent(0.0, 0.0, 10.0, 5.0),1.0,1.0)\n",
       "layoutDefinition = GridExtent(Extent(0.0, 0.0, 10.0, 5.0),1.0,1.0)\n",
       "pointCloud = MapPartitionsRDD[62] at flatMap at <console>:103\n",
       "pointCloudTiled = ContextRDD[65] at RDD at ContextRDD.scala:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ContextRDD[65] at RDD at ContextRDD.scala:35"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val extent = Extent(0, 0, 10, 5)\n",
    "val gridExtent = GridExtent(extent, 1, 1)  // 10×5 pixels\n",
    "\n",
    "//val layoutDefinition = LayoutDefinition(extent, TileLayout(layoutCols = 5, layoutRows = 10, tileCols = 10, tileRows = 10))\n",
    "val layoutDefinition = LayoutDefinition(gridExtent, 10, 5)\n",
    "val pointCloud = rdd_laz.flatMap(_._2)\n",
    "val pointCloudTiled = CutPointCloud(pointCloud, layoutDefinition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 41:====================================================>(993 + 7) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "points2 = MapPartitionsRDD[83] at mapPartitions at <console>:97\n",
       "pointsb = MapPartitionsRDD[88] at filter at <console>:104\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[88] at filter at <console>:104"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val points2 :RDD[(SpatialKey, Array[(Double, Double, Double, Byte, Double)])] = pointCloudTiled.mapPartitions{ _.map { case (s, packedPoints) => (s,{\n",
    "  var res = new Array[(Double, Double, Double, Byte, Double)](packedPoints.length)\n",
    "  cfor(0)(_ < packedPoints.length, _ + 1) { i =>\n",
    "    res(i) = (packedPoints.getX(i), packedPoints.getY(i), packedPoints.getZ(i), packedPoints.getByte(i, dim = \"Classification\"), packedPoints.getDouble(i, dim = \"HeightAboveGround\"))\n",
    "  }\n",
    "  res})\n",
    "}}\n",
    "val pointsb = points2.sortByKey().flatMap(m => m._2).filter(_._4 != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 32:====================================================>(995 + 5) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15785689"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointsb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 81:====================================================>(996 + 4) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(125000.0,487503.61100000003,10.738,6,7.497999999999999), (125000.0,487512.52400000003,10.867,6,7.644000000000001), (125000.0,487522.228,10.901,6,9.167), (125000.0,488181.435,-0.47500000000000003,9,-0.8190000000000001), (125000.0,488317.335,-0.466,9,-0.020000000000000018), (125000.0,488351.775,-0.47400000000000003,9,-0.028000000000000025), (125000.0,488396.354,-0.47900000000000004,9,-0.03300000000000003), (125000.0,488475.981,-0.481,9,-0.08999999999999997), (125000.0,488810.282,8.955,1,8.016), (125000.0,488861.316,4.473,1,3.635)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointsb.sortBy{ case(x, y, z, c, zh) => (x,y)}.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify if the created file and uploaded to HDFS has Normalized Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "laz_path = /user/hadoop/ahn3/C_25EZ2_hag.laz\n",
       "rdd_laz = NewHadoopRDD[0] at newAPIHadoopRDD at HadoopPointCloudRDD.scala:76\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val laz_path = new Path(\"/user/hadoop/ahn3/C_25EZ2_hag.laz\")\n",
    "val rdd_laz = HadoopPointCloudRDD(laz_path)\n",
    "rdd_laz.cache()\n",
    "rdd_laz.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "points = MapPartitionsRDD[32] at filter at <console>:95\n",
       "points10 = Array()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val points :RDD[(Double, Double, Double, Byte)] = rdd_laz.flatMap(_._2).mapPartitions{ _.map { packedPoints =>\n",
    "  var res = new Array[(Double, Double, Double, Byte)](packedPoints.length)\n",
    "  cfor(0)(_ < packedPoints.length, _ + 1) { i =>\n",
    "    res(i) = (packedPoints.getX(i), packedPoints.getY(i), packedPoints.getZ(i), packedPoints.getByte(i, dim = \"Classification\"))\n",
    "  }\n",
    "  res\n",
    "}}.flatMap( m => m).filter(_._4 != 2).filter(_._3 > 0)\n",
    "val points10 = points.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output = Emit(WrappedArray(env, None, (AWS_ACCESS_KEY_ID,A24H1RIGV4RKFGXJTEMS), (AWS_SECRET_ACCESS_KEY,5jd7ARCOi/XVjLzXqT5wA1NSgjmUo9mYJBgyGyIh)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Emit(WrappedArray(env, None, (AWS_ACCESS_KEY_ID,A24H1RIGV4RKFGXJTEMS), (AWS_SECRET_ACCESS_KEY,5jd7ARCOi/XVjLzXqT5wA1NSgjmUo9mYJBgyGyIh)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = sys. Process(\"env\",\n",
    "                     None,\n",
    "                     \"AWS_ACCESS_KEY_ID\" -> \"A24H1RIGV4RKFGXJTEMS\",\n",
    "                     \"AWS_SECRET_ACCESS_KEY\" -> \"5jd7ARCOi/XVjLzXqT5wA1NSgjmUo9mYJBgyGyIh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3_laz_path = ahn3\n",
       "s3_files = C_25EZ2.laz\n",
       "s3_file_out = C_25EZ2_hag.laz\n",
       "pipelineExpr = List(Read(local,None,None,None), HagFilter(filters.hag))\n",
       "s3_rdd_laz = NewHadoopRDD[1] at newAPIHadoopRDD at S3PointCloudRDD.scala:88\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NewHadoopRDD[1] at newAPIHadoopRDD at S3PointCloudRDD.scala:88"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s3_laz_path = \"ahn3\"\n",
    "val s3_files = \"C_25EZ2.laz\"\n",
    "val s3_file_out = \"C_25EZ2_hag.laz\"\n",
    "\n",
    "val pipelineExpr = Read(\"local\") ~ HagFilter()//~ LasWrite(s3_laz_path + \"C_25EZ2_hag.laz\")\n",
    "\n",
    "val s3_rdd_laz = S3PointCloudRDD(s3_laz_path, s3_files, options = S3PointCloudRDD.Options(pipeline = pipelineExpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: com.amazonaws.services.s3.model.AmazonS3Exception\n",
       "Message: The AWS Access Key Id you provided does not exist in our records. (Service: Amazon S3; Status Code: 403; Error Code: InvalidAccessKeyId; Request ID: 4D69CC537F251B7B; S3 Extended Request ID: bWyin9n4shEdxKOTQXM9mc2CyJHVRYe7cTMGCgms+hhJnElVFVcNoN9KaddQ9U17/LXkRa1Hoio=)\n",
       "StackTrace:   at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1632)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1304)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1058)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:743)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:717)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667)\n",
       "  at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649)\n",
       "  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513)\n",
       "  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4365)\n",
       "  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4312)\n",
       "  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4306)\n",
       "  at com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:840)\n",
       "  at geotrellis.spark.io.s3.AmazonS3Client.listObjects(AmazonS3Client.scala:45)\n",
       "  at geotrellis.spark.io.s3.S3Client$$anon$1.<init>(S3Client.scala:113)\n",
       "  at geotrellis.spark.io.s3.S3Client$class.listObjectsIterator(S3Client.scala:112)\n",
       "  at geotrellis.spark.io.s3.AmazonS3Client.listObjectsIterator(AmazonS3Client.scala:43)\n",
       "  at geotrellis.spark.io.s3.S3InputFormat.getSplits(S3InputFormat.scala:127)\n",
       "  at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:127)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1162)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_rdd_laz.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "extent = Extent(0.0, 0.0, 10.0, 5.0)\n",
       "gridExtent = GridExtent(Extent(0.0, 0.0, 10.0, 5.0),1.0,1.0)\n",
       "layoutDefinition = GridExtent(Extent(0.0, 0.0, 10.0, 5.0),1.0,1.0)\n",
       "pointCloud = MapPartitionsRDD[1] at flatMap at <console>:71\n",
       "pointCloudTiled = ContextRDD[4] at RDD at ContextRDD.scala:35\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ContextRDD[4] at RDD at ContextRDD.scala:35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val extent = Extent(0, 0, 10, 5)\n",
    "val gridExtent = GridExtent(extent, 1, 1)  // 10×5 pixels\n",
    "\n",
    "//val layoutDefinition = LayoutDefinition(extent, TileLayout(layoutCols = 5, layoutRows = 10, tileCols = 10, tileRows = 10))\n",
    "val layoutDefinition = LayoutDefinition(gridExtent, 10, 5)\n",
    "val pointCloud = s3_rdd_laz.flatMap(_._2)\n",
    "val pointCloudTiled = CutPointCloud(pointCloud, layoutDefinition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sonnets = s3a://files/sonnets.txt MapPartitionsRDD[1] at textFile at <console>:47\n",
       "counts = ShuffledRDD[4] at reduceByKey at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[4] at reduceByKey at <console>:48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sonnets = sc.textFile(\"s3a://files/sonnets.txt\")\n",
    "val counts = sonnets.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n",
    "//counts.saveAsTextFile(\"s3a://files/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalAccessError\n",
       "Message: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapred.FileInputFormat\n",
       "StackTrace:   at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:312)\n",
       "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1162)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
