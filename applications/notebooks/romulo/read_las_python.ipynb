{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Read a LAS file\n",
    "\n",
    "With this example the reader reads a LAS file from remote storage and stores it into a DataFrame in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Add all dependencies to PYTHON_PATH\n",
    "import sys\n",
    "sys.path.append(\"/usr/lib/spark/python\")\n",
    "sys.path.append(\"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"/usr/lib/python3/dist-packages\")\n",
    "\n",
    "#Define environment variables\n",
    "import os\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"ipython\"\n",
    "os.environ[\"PATH\"] = \"/data/local/jupyterhub/modules/LAStools/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "#Load PySpark to connect to a Spark cluster\n",
    "from pyspark import sql, SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "#To read GeoTiffs as a ByteArray\n",
    "from io import BytesIO\n",
    "\n",
    "from laspy.file import File\n",
    "import urllib  # the lib that handles the url stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  new Spark Context will be created.\n"
     ]
    }
   ],
   "source": [
    "appName = \"read LAS file\"\n",
    "masterURL=\"spark://emma0.emma.nlesc.nl:7077\"\n",
    "\n",
    "#A context needs to be created if it does not already exist\n",
    "try:\n",
    "    saprk.stop()\n",
    "except NameError:\n",
    "    print(\"A  new Spark Context will be created.\")\n",
    "\n",
    "spark = sql.SparkSession.builder.appName(appName).master(masterURL).config(\"parquet.enable.dictionary\", \"true\").config(\"parquet.compression\", \"SNAPPY\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_url = 'https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz/C_25EZ2.LAZ'\n",
    "#data = urllib.request.urlopen(las_url) # it's a file like object and works just like a file\n",
    "file_path = '/data/local/jupyterhub/C_25EZ2.laz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "las_file = Path(file_path)\n",
    "if (las_file.is_file() == False):\n",
    "    r = requests.get(las_url, allow_redirects=True)\n",
    "    open(file_path, 'wb').write(r.content)\n",
    "\n",
    "inFile = File(file_path, mode='r')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = np.array(['x', 'y', 'z', 'intensity', 'fkag_byte', 'raw_classification', 'scan_angle_rank', 'user_data', 'pt_src_id', 'gps_time'])\n",
    "cols = np.array(['x', 'y', 'z', 'intensity', 'raw_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.DataFrame(columns=cols, dtype=int32)\n",
    "dataset = pd.DataFrame()#dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.assign(**{\n",
    "#    'x' : inFile.X, \n",
    "#    'y' : inFile.X, \n",
    "#    'z' : inFile.X, \n",
    "#    'intensity' : inFile.intensity,\n",
    "#    'flag_byte' : inFile.flag_byte,\n",
    "#    'raw_classfication' : inFile.raw_classification,\n",
    "#    'user_data' : inFile.user_data,\n",
    "#    'pt_src_id' : inFile.pt_src_id,\n",
    "#    'gps_time' : inFile.gps_time\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.assign(**{\n",
    "    'x' : inFile.X, \n",
    "    'y' : inFile.X, \n",
    "    'z' : inFile.X, \n",
    "    'intensity' : inFile.intensity,\n",
    "    'raw_classification' : inFile.raw_classification\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intensity             uint16\n",
       "raw_classification     uint8\n",
       "x                      int32\n",
       "y                      int32\n",
       "z                      int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasSchema = StructType([\n",
    "    StructField(\"x\", IntegerType(), True),\n",
    "    StructField(\"y\", IntegerType(), True),\n",
    "    StructField(\"z\", IntegerType(), True),\n",
    "    StructField(\"intensity\", ShortType(), True),\n",
    "    StructField(\"raw_classification\", ByteType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sqlCtx.createDataFrame(dataset, samplingRatio=None, verifySchema=False, schema=lasSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.parquet(\"/user/emma/C_25EZ2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 'int'),\n",
       " ('y', 'int'),\n",
       " ('z', 'int'),\n",
       " ('intensity', 'smallint'),\n",
       " ('raw_classification', 'tinyint')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
