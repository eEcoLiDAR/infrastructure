{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames and Magellan\n",
    "\n",
    "We advise the read of the [sql programming guide/sql](https://spark.apache.org/docs/2.1.1/sql-programming-guide.html#sql) and visit [Magellan repository](https://github.com/harsha2010/magellan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.types.{DoubleType, StructField, StructType}\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@21b85e2a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@21b85e2a"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dir_path = hdfs:///user/emma/ecolidar/\n",
       "offline_dir_path = hdfs:///user/emma/ecolidar/\n",
       "las_cell = C_25EZ2\n",
       "parquet_file = C_25EZ2.parquet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "C_25EZ2.parquet"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dir_path = \"hdfs:///user/emma/ecolidar/\"\n",
    "var offline_dir_path = \"hdfs:///user/emma/ecolidar/\"\n",
    "var las_cell = \"C_25EZ2\"\n",
    "var parquet_file = las_cell + \".parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrame\n",
    "\n",
    "The dataframe is loaded from a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [x: int, y: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[x: int, y: int ... 3 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sqlContext.read.parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><td>712</td><td>2</td><td>125667105</td><td>-30943</td><td>33</td></tr>\n",
       "<tr><td>736</td><td>2</td><td>125667415</td><td>-30633</td><td>87</td></tr>\n",
       "<tr><td>678</td><td>2</td><td>125667715</td><td>-30333</td><td>-125</td></tr>\n",
       "<tr><td>813</td><td>2</td><td>125668046</td><td>-30002</td><td>-50</td></tr>\n",
       "<tr><td>900</td><td>2</td><td>125668356</td><td>-29692</td><td>4</td></tr>\n",
       "<tr><td>774</td><td>2</td><td>125668657</td><td>-29391</td><td>49</td></tr>\n",
       "<tr><td>736</td><td>2</td><td>125668940</td><td>-29108</td><td>76</td></tr>\n",
       "<tr><td>749</td><td>2</td><td>125669238</td><td>-28810</td><td>118</td></tr>\n",
       "<tr><td>800</td><td>2</td><td>125669528</td><td>-28520</td><td>-104</td></tr>\n",
       "<tr><td>724</td><td>2</td><td>125669852</td><td>-28196</td><td>-36</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----+-----+-----------+--------+------+\n",
       "| 712 | 2   | 125667105 | -30943 | 33   |\n",
       "| 736 | 2   | 125667415 | -30633 | 87   |\n",
       "| 678 | 2   | 125667715 | -30333 | -125 |\n",
       "| 813 | 2   | 125668046 | -30002 | -50  |\n",
       "| 900 | 2   | 125668356 | -29692 | 4    |\n",
       "| 774 | 2   | 125668657 | -29391 | 49   |\n",
       "| 736 | 2   | 125668940 | -29108 | 76   |\n",
       "| 749 | 2   | 125669238 | -28810 | 118  |\n",
       "| 800 | 2   | 125669528 | -28520 | -104 |\n",
       "| 724 | 2   | 125669852 | -28196 | -36  |\n",
       "+-----+-----+-----------+--------+------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Temporary table or view\n",
    "\n",
    "It is possible to create a temporary table or view from a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tempTab: Unit = ()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(las_cell + \"_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show existent tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |c_25ez2_tab|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a simple SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res = [x: int, y: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[x: int, y: int ... 3 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val res = sqlContext.sql(\"select * from \" + las_cell + \"_tab limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+---------+------------------+\n",
      "|  x|  y|        z|intensity|raw_classification|\n",
      "+---+---+---------+---------+------------------+\n",
      "|712|  2|125667105|   -30943|                33|\n",
      "|736|  2|125667415|   -30633|                87|\n",
      "|678|  2|125667715|   -30333|              -125|\n",
      "|813|  2|125668046|   -30002|               -50|\n",
      "|900|  2|125668356|   -29692|                 4|\n",
      "|774|  2|125668657|   -29391|                49|\n",
      "|736|  2|125668940|   -29108|                76|\n",
      "|749|  2|125669238|   -28810|               118|\n",
      "|800|  2|125669528|   -28520|              -104|\n",
      "|724|  2|125669852|   -28196|               -36|\n",
      "+---+---+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magellan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magellan.{Point, Polygon}\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            point|\n",
      "+-----------------+\n",
      "|Point(-1.0, -1.0)|\n",
      "| Point(-1.0, 1.0)|\n",
      "| Point(1.0, -1.0)|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "points = [point: point]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[point: point]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Only 2D points are supported.\n",
    "val points = sc.parallelize(Seq((-1.0, -1.0), (-1.0, 1.0), (1.0, -1.0))).toDF(\"x\", \"y\").select(point($\"x\", $\"y\").as(\"point\"))\n",
    "\n",
    "points.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a list of points from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            point|\n",
      "+-----------------+\n",
      "|Point(712.0, 2.0)|\n",
      "|Point(736.0, 2.0)|\n",
      "|Point(678.0, 2.0)|\n",
      "|Point(813.0, 2.0)|\n",
      "|Point(900.0, 2.0)|\n",
      "|Point(774.0, 2.0)|\n",
      "|Point(736.0, 2.0)|\n",
      "|Point(749.0, 2.0)|\n",
      "|Point(800.0, 2.0)|\n",
      "|Point(724.0, 2.0)|\n",
      "|Point(622.0, 2.0)|\n",
      "|Point(576.0, 2.0)|\n",
      "|Point(724.0, 2.0)|\n",
      "|Point(761.0, 2.0)|\n",
      "|Point(690.0, 2.0)|\n",
      "|Point(724.0, 2.0)|\n",
      "|Point(775.0, 2.0)|\n",
      "|Point(737.0, 2.0)|\n",
      "|Point(737.0, 2.0)|\n",
      "|Point(632.0, 1.0)|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lidar_points = [point: point]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[point: point]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lidar_points = df.select(point($\"x\", $\"y\").as(\"point\"))\n",
    "\n",
    "lidar_points.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
